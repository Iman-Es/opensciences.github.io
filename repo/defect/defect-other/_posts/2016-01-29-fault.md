---
title: Fault
excerpt: "Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges"
layout: repo-dataset
authors: "Shamshiri, Sina, Rene Just, Jose Miguel Rojas, Gordon Fraser, Phil McMinn, and Andrea Arcuri"
version: 4
---

#URL

* [Data in tera-PROMISE](https://terapromise.csc.ncsu.edu:8443/!/#repo/view/head/defect/defect-other/fault)
* [Paper in IEEE Digital Library](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7372009&punumber%3D7371449%26filter%3DAND%28p_IS_Number%3A7371976%29%26pageNumber%3D2)

#Change Log

When | What
---- | ----
January 29th, 2016 | Donated by [Rene Just](mailto:rjust@cs.washington.edu)

#Reference

Studies who have been using the data (in any form) are required to include the following reference:

```
@inproceedings{JustJE2014,
   author = {Rene Just and Darioush Jalali and Michael D. Ernst},
   title = {Defects4J: A database of existing faults to enable
	controlled testing studies for {J}ava programs},
   booktitle = {Proceedings of the International Symposium on Software
	Testing and Analysis (ISSTA)},
   pages = {437--440},
   address = {San Jose, CA, USA},
   month = {July~23--25},
   year = {2014}
}   
```

#About the Data

##Overview of Data

Defects4J: A Database of Existing Faults to
Enable Controlled Testing Studies for Java Programs

##Paper Abstract

Rather than tediously writing unit tests manually, tools can be used to generate them automatically -- sometimes even resulting in higher code coverage than manual testing. But how good are these tests at actually finding faults? To answer this question, we applied three state-of-the-art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. Although the automatically generated test suites detected 55.7% of the faults overall, only 19.9% of all the individual test suites detected a fault. By studying the effectiveness and problems of the individual tools and the tests they generate, we derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. These insights include 1) improving the obtained code coverage so that faulty statements are executed in the first instance, 2) improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, and 3) improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time.